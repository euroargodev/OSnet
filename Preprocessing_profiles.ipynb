{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb92b9-15b5-4ed1-abe5-73b2307465b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing of CORA dataset\n",
    "#date : February 2022\n",
    "#author : Etienne Pauthenet (etienne.pauthenet@gmail.com) & Loic Bachelot (loic.bachelot@gmail.com)\n",
    "import datetime as dt\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import xarray as xr\n",
    "from gsw import sigma0\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b6b64-c584-4fb2-a3ee-9fd8c8c53aaf",
   "metadata": {},
   "source": [
    "# Preselection of the Raw CORA profiles (INSITU_GLO_TS_REP_OBSERVATIONS_013_001_b)\n",
    "- CORA raw files without moorings MO.nc and drifting buoys DB.nc and TE.nc\n",
    "- select  profiles that contain PRES, TEMP and PSAL \n",
    "- adjusted variable if it exists\n",
    "- QC = 1 only for the three variables combined TEMP, PSAL and PRES\n",
    "- Gulf Stream region\n",
    "- Save the selected profiles in daily xarray dataset and netcdf (TEMP, PSAL, PRES, LON, LAT, JULD (ref to 1950-01-01), DC_REFERENCE, PLATFORM_NUMBER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c176e-dbb0-42e0-bc9f-80a5997d0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cora_rep = ''\n",
    "data_rep = ''\n",
    "for yy in range(1990,2021):\n",
    "    listfile = set(glob.glob(cora_rep + str(yy) + '/CO_*.nc')) - set(glob.glob(cora_rep + str(yy) + '/*MO.nc')) - set(glob.glob(cora_rep + str(yy) + '/*TE.nc')) - set(glob.glob(cora_rep + str(yy) + '/*DB.nc'))\n",
    "    count = list()\n",
    "    for myfile in listfile:\n",
    "        filename = os.path.basename(myfile)\n",
    "        ds = nc.Dataset(myfile, 'r')  # r+\n",
    "        dm = ds.variables['DATA_MODE'][:]\n",
    "        dcref = ds.variables['DC_REFERENCE'][:,:]\n",
    "        lon   = ds.variables['LONGITUDE'][:]\n",
    "        lat   = ds.variables['LATITUDE'][:]\n",
    "        juld  = ds.variables['JULD'][:]\n",
    "        platform_number = ds.variables['PLATFORM_NUMBER'][:]\n",
    "        ind     = list()\n",
    "        mydcref = list()\n",
    "        mypfnum = list()\n",
    "        for ii in range(0,len(dm)):\n",
    "            suffix  =''\n",
    "            #REGION\n",
    "            if lon[ii]>=-80 and lon[ii]<=-30 and lat[ii]>=23 and lat[ii]<=50:\n",
    "                #ADJUSTED IF IT EXIST\n",
    "                if dm[ii] == b'A' or dm[ii] == b'D':\n",
    "                    suffix = '_ADJUSTED'\n",
    "                #TEMP AND PSAL AND PRES\n",
    "                if 'TEMP' in ds.variables.keys() and 'PSAL' in ds.variables.keys() and 'PRES' in ds.variables.keys():\n",
    "                    ind.append(ii)\n",
    "                    dc = b''.join(dcref[ii,dcref[ii,:].mask== False])\n",
    "                    dc = dc.decode('utf-8')\n",
    "                    mydcref.append(dc)\n",
    "                    pf = b''.join(platform_number[ii,platform_number[ii,:].mask== False])\n",
    "                    mypfnum.append(pf)\n",
    "        if len(ind)>0:\n",
    "            count.append(len(ind))\n",
    "            #RETRIEVE QC FOR ind AND CREATE MASK FROM IT\n",
    "            mask_TEMP = np.isin(ds.variables['TEMP' + suffix  + '_QC'][ind,:].data, b'1')\n",
    "            mask_PSAL = np.isin(ds.variables['PSAL' + suffix  + '_QC'][ind,:].data, b'1')\n",
    "            mask_PRES = np.isin(ds.variables['PRES' + suffix  + '_QC'][ind,:].data, b'1')\n",
    "            #MAKE A SINGLE MASK FROM THE COMBINED THREE OTHERS\n",
    "            mask = mask_TEMP & mask_PSAL & mask_PRES\n",
    "            #EXPORT IN DATASET XARRAY\n",
    "            dsxr = xr.Dataset(\n",
    "            data_vars=dict(\n",
    "                TEMP = ([\"N_PROF\",\"N_PRES\"], ma.masked_array(ds.variables['TEMP' + suffix ][ind,:], mask=~mask)),\n",
    "                PSAL = ([\"N_PROF\",\"N_PRES\"], ma.masked_array(ds.variables['PSAL' + suffix ][ind,:], mask=~mask)),\n",
    "                PRES = ([\"N_PROF\",\"N_PRES\"], ma.masked_array(ds.variables['PRES' + suffix ][ind,:], mask=~mask)),\n",
    "                DC_REFERENCE    = ([\"N_PROF\"], mydcref),\n",
    "                PLATFORM_NUMBER = ([\"N_PROF\"], mypfnum)\n",
    "            ),\n",
    "            coords=dict(\n",
    "                LON  = ([\"N_PROF\"], lon[ind]),\n",
    "                LAT  = ([\"N_PROF\"], lat[ind]),\n",
    "                JULD = ([\"N_PROF\"],juld[ind])\n",
    "            ),\n",
    "            attrs=dict(description=\"CORA subsampled : Selection of profiles with (1) TEMP, PSAL and PRES present, (2) adjusted data if adjusted exist and (3) in the Gulf Stream region (lon in [-80,-30], lat in [23,50]\"),\n",
    "            )\n",
    "            dsxr.JULD.attrs[\"units\"] = \"days since 1950-01-01\"\n",
    "            dsxr['DC_REFERENCE'] = dsxr['DC_REFERENCE'].astype('|S8')\n",
    "            dsxr.to_netcdf(data_rep + myfile[50:76] + \"_GulfStream.nc\")\n",
    "        ds.close()\n",
    "    print('Extracted a total of ' + str(sum(count)) + ' TS profiles in the Gulf Stream Region, for the year ' + str(yy) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76677be4-fea4-4d57-b462-1e980920c870",
   "metadata": {},
   "source": [
    "# Interpolate each profiles on a regular vertical grid (All of that is done on the script interp_cora.py)\n",
    "- Remove data deeper than 1000m\n",
    "- Interpolate each profiles with np.interp() on 51 depth levels. Fill to the surface with the last non NaN value and fill with NaN to the bottom. Save also a version with NaN at the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8193e2f-ef13-41a4-b1ca-3d7b1dec6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pi = [0, 1, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 19, 22, 26, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 90, 100, 110, 120, 133, 147, 163, 180, 199, 221, 245, 271,\n",
    "                301, 334, 371, 412, 458, 509, 565, 628, 697, 773, 857, 950, 1000]\n",
    "listfile = glob.glob('*.nc')\n",
    "total_files = len(listfile)\n",
    "datasets = []\n",
    "nb_prof = 0\n",
    "count = 0\n",
    "\n",
    "for myfile in listfile:\n",
    "    ds = xr.open_dataset(myfile)\n",
    "    ds = ds.dropna('N_PROF', how = \"all\", subset=['TEMP', 'PSAL'])\n",
    "    N = len(ds.N_PROF)\n",
    "    #Exclude empty files\n",
    "    if N>0:\n",
    "        TEMPi = np.empty((N,len(pi)))\n",
    "        TEMPi[:] = np.nan\n",
    "        PSALi = np.empty((N,len(pi)))\n",
    "        PSALi[:] = np.nan\n",
    "        TEMPs = np.empty((N,len(pi)))\n",
    "        TEMPs[:] = np.nan\n",
    "        PSALs = np.empty((N,len(pi)))\n",
    "        PSALs[:] = np.nan\n",
    "        for n in range(0,N):\n",
    "            TEMPi[n,:] = np.interp(pi, ds.PRES[n,:], ds.TEMP[n,:],right=np.nan)\n",
    "            PSALi[n,:] = np.interp(pi, ds.PRES[n,:], ds.PSAL[n,:],right=np.nan)\n",
    "            TEMPs[n,:] = np.interp(pi, ds.PRES[n,:], ds.TEMP[n,:],left=np.nan,right=np.nan)\n",
    "            PSALs[n,:] = np.interp(pi, ds.PRES[n,:], ds.PSAL[n,:],left=np.nan,right=np.nan)\n",
    "            #EXPORT INTERPOLATED PROFILES\n",
    "        dsi = xr.Dataset(\n",
    "            data_vars=dict(\n",
    "                TEMP_SURF = ([\"N_PROF\",\"PRES_INTERPOLATED\"], TEMPs),\n",
    "                PSAL_SURF = ([\"N_PROF\",\"PRES_INTERPOLATED\"], PSALs),\n",
    "                TEMP_INTERP = ([\"N_PROF\",\"PRES_INTERPOLATED\"], TEMPi),\n",
    "                PSAL_INTERP = ([\"N_PROF\",\"PRES_INTERPOLATED\"], PSALi),\n",
    "                DC_REFERENCE    = ([\"N_PROF\"], ds.DC_REFERENCE.data),\n",
    "                PLATFORM_NUMBER = ([\"N_PROF\"], ds.PLATFORM_NUMBER.data)),\n",
    "            coords=dict(\n",
    "                LON  = ([\"N_PROF\"], ds.LON.data),\n",
    "                LAT  = ([\"N_PROF\"], ds.LAT.data),\n",
    "                PRES_INTERPOLATED  = ([\"PRES_INTERPOLATED\"], pi),\n",
    "                JULD = ([\"N_PROF\"],ds.JULD.data)))\n",
    "        datasets.append(dsi)\n",
    "        nb_prof = nb_prof + len(dsi.N_PROF)\n",
    "    count = count+1\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"{(count/total_files)*100}% done\")\n",
    "        print(f\"{nb_prof} profiles processed\")\n",
    "        \n",
    "print('finished, concatenation starting')\n",
    "ds_interp = xr.concat(datasets, dim='N_PROF')\n",
    "ds_interp.to_netcdf(data_rep + \"ds_interp.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf937a-f588-473d-b047-2da3fa9f5fff",
   "metadata": {},
   "source": [
    "# Cleaning and formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f414ae7-c498-44b7-8a0f-9f4ee5cb7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(data_rep +\"ds_interp.nc\")\n",
    "ds = ds.dropna('N_PROF', how = \"any\", subset=['TEMP_INTERP', 'PSAL_INTERP'])\n",
    "x = np.array(ds.TEMP_INTERP)\n",
    "print(f\"number of nan left: {np.count_nonzero(np.isnan(x))}\")\n",
    "\n",
    "#Select the period 1993-2019\n",
    "ds = ds.where(ds.JULD.dt.year<=2019,drop = True)\n",
    "ds = ds.where(ds.JULD.dt.year>=1993,drop = True)\n",
    "\n",
    "ds_clean = ds.where(np.isnan(ds['TEMP_SURF'].isel(PRES_INTERPOLATED=14))==False, drop=True)\n",
    "ds_clean = ds_clean.rename({'LAT': 'LATITUDE', 'LON':'LONGITUDE', 'JULD':'TIME'})\n",
    "ds_clean.to_netcdf(data_rep + \"CORA1000_1993-2019.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061a9ee-a7f6-4c78-9499-f51e2781a97a",
   "metadata": {},
   "source": [
    "# Horizontal interpolation of surface data at the CORA profile's location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be108635-bd3a-4606-a8bc-42424169c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "ds = xr.open_dataset(data_rep + \"CORA1000_1993-2019.nc\")\n",
    "lats = ds.LATITUDE\n",
    "lons = ds.LONGITUDE\n",
    "dates = ds.TIME\n",
    "\n",
    "#BATHYMETRY\n",
    "ds_bat = xr.open_dataset('bathymetry_GulfStream.nc')\n",
    "res_lin = ds_bat.interp(LATITUDE=lats,LONGITUDE=lons,method = 'linear')['bathymetry'].values\n",
    "ds = ds.assign(variables={\"bathy\": (('N_PROF'), res_lin)})\n",
    "\n",
    "#MDT\n",
    "ds_mdt = xr.open_dataset('mdt-cnes-cls18-global.nc')\n",
    "ds_mdt = ds_mdt.assign_coords(lon180=(((ds_mdt.longitude + 180) % 360) - 180))  \n",
    "ds_mdt['longitude'] = ds_mdt.lon180\n",
    "res = ds_mdt.interp(latitude=lats,longitude=lons,method = 'nearest')['mdt'].values[0]\n",
    "ds = ds.assign(variables={\"MDT\": (('N_PROF'), res)})\n",
    "\n",
    "#SST\n",
    "path = '/home/datawork-lops-bluecloud/osnet/data_remote_sensing/SST/SST_Gulf_Stream/'\n",
    "ds_sst = xr.open_mfdataset(path + '/*.nc')\n",
    "spatial_domain = {\"lon\":[-80, -30],\n",
    "                 \"lat\": [23, 50]}\n",
    "ds_sst = ds_sst.sel(lon=slice(spatial_domain['lon'][0], spatial_domain['lon'][1]),\n",
    "                    lat=slice(spatial_domain['lat'][0], spatial_domain['lat'][1]))\n",
    "res = ds_sst.sel(dict(lat=lats, lon=lons, time=dates), method='nearest')['analysed_sst'].values-273.15\n",
    "ds = ds.assign(variables={\"SST\": (('N_PROF'), res)})\n",
    "res = ds_sst.sel(dict(lat=lats, lon=lons, time=dates), method='nearest')['analysis_uncertainty'].values\n",
    "ds = ds.assign(variables={\"SST_uncertainty\": (('N_PROF'), res)})\n",
    "\n",
    "#Find NaN if any\n",
    "# In the case of SST, we identified the error being for profiles sampled after 2019-12-31T12:00:00. \n",
    "# The xr.interp() function is not assigning properly these profiles and creating NaN. So we change the date of these profiles to 2019-12-31T12:00:00.\n",
    "sst = np.array(ds.SST_uncertainty)\n",
    "indna = np.argwhere(np.isnan(sst))[:,0]\n",
    "indna\n",
    "\n",
    "if len(indna)>0:\n",
    "    print(f\"{len(indna)} prof to fix\")\n",
    "    lats3  = ds.LATITUDE.isel(N_PROF = indna)\n",
    "    lons3  = ds.LONGITUDE.isel(N_PROF = indna)\n",
    "    dates3 = np.datetime64('2019-12-31T12:00:00.000000000')\n",
    "else:\n",
    "    print(\"no prof to fix\")\n",
    "\n",
    "if len(indna)>0:\n",
    "    res = ds_sst.interp(lat=lats3,lon=lons3,time = dates3,method = 'nearest')['analysed_sst'].values-273.15\n",
    "    ds.SST[indna] = res\n",
    "    res = ds_sst.interp(lat=lats3,lon=lons3,time = dates3,method = 'nearest')['analysis_uncertainty'].values\n",
    "    ds.SST_uncertainty[indna] = res\n",
    "    ds\n",
    "    \n",
    "#SLA\n",
    "ds_sla = xr.open_mfdataset('*.nc')\n",
    "ds_sla = ds_sla.assign_coords(lon180=(((ds_sla.longitude + 180) % 360) - 180))  \n",
    "ds_sla['longitude'] = ds_sla.lon180 \n",
    "ds_sla = ds_sla.sortby('longitude')\n",
    "res = ds_sla.sel(dict(latitude=lats, longitude=lons, time=dates), method='nearest')\n",
    "ds = ds.assign(variables={\"SLA\": (('N_PROF'), res['sla'].values)}) \n",
    "ds = ds.assign(variables={\"UGOS\": (('N_PROF'), res['ugos'].values)}) \n",
    "ds = ds.assign(variables={\"VGOS\": (('N_PROF'), res['vgos'].values)}) \n",
    "ds = ds.assign(variables={\"UGOSA\": (('N_PROF'), res['ugosa'].values)}) \n",
    "ds = ds.assign(variables={\"VGOSA\": (('N_PROF'), res['vgosa'].values)}) \n",
    "ds = ds.assign(variables={\"SLA_err\": (('N_PROF'), res['err'].values)}) \n",
    "\n",
    "#Find NaN if any\n",
    "#Here we replace by a nearest neighbour  24.875 and -77.625 (The profiles wtih NaN inputs where close to this grid cell)\n",
    "UGOS = np.array(ds.UGOS)\n",
    "indna = np.argwhere(np.isnan(UGOS))[:,0]\n",
    "indna\n",
    "if len(indna)>0:\n",
    "    print(f\"{len(indna)} prof to fix\")\n",
    "    lats_nan  = 24.875\n",
    "    lons_nan  = -77.625\n",
    "    dates_nan = ds.TIME.isel(N_PROF = indna)\n",
    "else:\n",
    "    print(\"no prof to fix\")\n",
    "\n",
    "if len(indna)>0:\n",
    "    res = ds_sla.sel(dict(latitude=lats_nan, longitude=lons_nan, time=dates_nan), method='nearest')\n",
    "    ds.UGOS[indna] = res.ugos\n",
    "    ds.UGOSA[indna] = res.ugosa\n",
    "    ds.VGOS[indna] = res.vgos\n",
    "    ds.VGOSA[indna] = res.vgosa\n",
    "\n",
    "#Check for NaNs\n",
    "for var in [\"UGOS\", \"UGOSA\", \"VGOS\", \"VGOSA\"]:\n",
    "    tmp = np.array(ds[var])\n",
    "    indna = np.argwhere(np.isnan(tmp))[:,0]\n",
    "    print(f\"var name {var}: {len(indna)}\")\n",
    "\n",
    "#Save\n",
    "ds.to_netcdf(data_rep + \"CORA1000_1993-2019_coloc.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863cdbe-6195-4c8e-808a-85fbba876ba1",
   "metadata": {},
   "source": [
    "# Rename and add sigma0 and MLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d30a7-8d93-44c5-9b5b-2036bd381d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "ds.to_netcdf(data_rep + \"CORA1000_1993-2019_coloc.nc\")\n",
    "\n",
    "# reformating and renaming\n",
    "ds = ds.rename({\"TEMP_INTERP\": \"TEMP\", \"PSAL_INTERP\": \"PSAL\"})\n",
    "\n",
    "SA = gsw.SA_from_SP(ds['PSAL'], ds['PRES_INTERPOLATED'], ds['LONGITUDE'], ds['LATITUDE'])\n",
    "CT = gsw.CT_from_t(SA, ds['TEMP'], ds['PRES_INTERPOLATED'])\n",
    "SIG = gsw.sigma0(SA, CT)\n",
    "ds = ds.assign(variables={\"SA\": (('N_PROF', 'PRES_INTERPOLATED'), SA.data)})\n",
    "ds = ds.assign(variables={\"CT\": (('N_PROF', 'PRES_INTERPOLATED'), CT.data)})\n",
    "ds = ds.assign(variables={\"SIG\": (('N_PROF', 'PRES_INTERPOLATED'), SIG.data)})\n",
    "\n",
    "sig_diff = ds.SIG - ds.SIG.sel(PRES_INTERPOLATED = 10)-0.03\n",
    "MLD_obs = ds['PRES_INTERPOLATED'].where(sig_diff>0).min(dim='PRES_INTERPOLATED')\n",
    "ds = ds.assign(MLD = MLD_obs)\n",
    "ds.to_netcdf(data_rep + \"CORA1000_1993-2019_clean.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
